{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ed2b199-bc54-44a7-a517-fa6529f6214b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama-python in /opt/conda/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (6.0.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (2.20.0)\n",
      "Requirement already satisfied: langdetect in /opt/conda/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.11/site-packages (0.24.2)\n",
      "Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.43.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.11/site-packages (3.11.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (4.66.4)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.11/site-packages (17.0.0)\n",
      "Requirement already satisfied: IProgress in /opt/conda/lib/python3.11/site-packages (0.4)\n",
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.11/site-packages (3.5.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.5.1)\n",
      "Requirement already satisfied: hf-transfer in /opt/conda/lib/python3.11/site-packages (0.1.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (6.0.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib==3.7.3 in /opt/conda/lib/python3.11/site-packages (3.7.3)\n",
      "Requirement already satisfied: fasttext in /opt/conda/lib/python3.11/site-packages (0.9.3)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: cupy-cuda12x in /opt/conda/lib/python3.11/site-packages (13.2.0)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: umap-learn in /opt/conda/lib/python3.11/site-packages (0.5.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.7.3) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.7.3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.7.3) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.7.3) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.7.3) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.7.3) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.7.3) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.7.3) (2.8.2)\n",
      "Requirement already satisfied: httpx<0.27.0,>=0.26.0 in /opt/conda/lib/python3.11/site-packages (from ollama-python) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.3 in /opt/conda/lib/python3.11/site-packages (from ollama-python) (2.7.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /opt/conda/lib/python3.11/site-packages (from ollama-python) (2.32.3)\n",
      "Requirement already satisfied: responses<0.25.0,>=0.24.1 in /opt/conda/lib/python3.11/site-packages (from ollama-python) (0.24.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.15.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub) (4.12.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.11/site-packages (from fasttext) (2.12.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from fasttext) (68.2.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /opt/conda/lib/python3.11/site-packages (from cupy-cuda12x) (0.8.2)\n",
      "Requirement already satisfied: numba>=0.51.2 in /opt/conda/lib/python3.11/site-packages (from umap-learn) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.11/site-packages (from umap-learn) (0.5.12)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<0.27.0,>=0.26.0->ollama-python) (4.0.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<0.27.0,>=0.26.0->ollama-python) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<0.27.0,>=0.26.0->ollama-python) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<0.27.0,>=0.26.0->ollama-python) (3.4)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from httpx<0.27.0,>=0.26.0->ollama-python) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.27.0,>=0.26.0->ollama-python) (0.14.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.11/site-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.3->ollama-python) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.3->ollama-python) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->ollama-python) (3.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->ollama-python) (2.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "##Primary Install\n",
    "\n",
    "!sudo apt update -y\n",
    "\n",
    "!sudo apt install default-jdk -y\n",
    "\n",
    "!sudo apt install build-essential -y\n",
    "\n",
    "!pip install ollama-python psutil datasets langdetect huggingface-hub sentence-transformers transformers pandas h5py tqdm pyarrow IProgress pyspark scikit-learn hf-transfer pyyaml numpy matplotlib==3.7.3 fasttext torch cupy-cuda12x seaborn umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a3f494b-2192-42a0-b8fa-3f83f86f955f",
   "metadata": {
    "id": "6a3f494b-2192-42a0-b8fa-3f83f86f955f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Checked and/or created default configuration files in ./config_files\n",
      "Full_Configuration:\n",
      "\n",
      "User Configuration:\n",
      "User: TheSkullery\n",
      "Merged_Dataset_name: Aether-Lite-v2\n",
      "\n",
      "System Configuration:\n",
      "  Key Name: conversations\n",
      "  CPU Threads Used: 20\n",
      "  Chunk Size: 10000\n",
      "  Processed Chunks Directory: ./processed_chunks\n",
      "  Script Version: DMKv3\n",
      "  Rewrite System Prompt: True\n",
      "  System Prompt Minimum Token Length: 50\n",
      "Configured Datasets:\n",
      "  - Name: jondurbin/airoboros-3.2\n",
      "    Use Percentage: 100%\n",
      "    Phrase List Key: 1\n",
      "  - Name: LLM-Experiments/Steel-Med\n",
      "    Use Percentage: 100%\n",
      "    Phrase List Key: 1\n",
      "  - Name: mpasila/no-robots-sharegpt-edit\n",
      "    Use Percentage: 100%\n",
      "    Phrase List Key: 1\n",
      "  - Name: nothingiisreal/Kalomaze-Opus-Instruct-25k-filtered\n",
      "    Use Percentage: 100%\n",
      "    Phrase List Key: 1\n",
      "  - Name: mrfakename/Pure-Dove-ShareGPT\n",
      "    Use Percentage: 100%\n",
      "    Phrase List Key: 1\n",
      "  - Name: Undi95/Capybara-ShareGPT\n",
      "    Use Percentage: 100%\n",
      "    Phrase List Key: 1\n",
      "  - Name: PJMixers/grimulkan_theory-of-mind-ShareGPT\n",
      "    Use Percentage: 100%\n",
      "    Phrase List Key: 1\n",
      "  - Name: PJMixers/grimulkan_physical-reasoning-ShareGPT\n",
      "    Use Percentage: 100%\n",
      "    Phrase List Key: 1\n",
      "  - Name: PJMixers/grimulkan_physical-reasoning-ShareGPT\n",
      "    Use Percentage: 100%\n",
      "    Phrase List Key: 1\n",
      "  - Name: TheSkullery/WizardLM_evol_instruct_v2_Filtered_Fuzzy_Dedup_ShareGPT\n",
      "    Use Percentage: 50%\n",
      "    Phrase List Key: 1\n",
      "  - Name: Undi95/andrijdavid_roleplay-conversation-sharegpt\n",
      "    Use Percentage: 100%\n",
      "    Phrase List Key: 2\n",
      "  - Name: TheSkullery/Gryphe-Opus-WritingPrompts-merged\n",
      "    Use Percentage: 100%\n",
      "    Phrase List Key: 2\n",
      "  - Name: mpasila/LimaRP-PIPPA-freedom-rp-Mix-8K\n",
      "    Use Percentage: 100%\n",
      "    Phrase List Key: 2\n",
      "  - Name: Alignment-Lab-AI/RPGuild-sharegpt-filtered\n",
      "    Use Percentage: 100%\n",
      "    Phrase List Key: 2\n",
      "  - Name: Steelskull/SLOP-MEGA\n",
      "    Use Percentage: 100%\n",
      "    Phrase List Key: 2\n",
      "\n",
      "\n",
      "Filtered Data Types: function-call, function-response, assistant\n",
      "\n",
      "acceptable_languages: ['en']\n",
      "\n",
      "Phrase Lists:\n",
      "  List 1: couldn't help but, can't resist, I'm sorry, but, I'm sorry but, as an AI, as a Language Model, AI Language Model, language model, However, it is important to, However, it's important, ethical guidelines, just an AI, within my programming, cannot provide, in conclusion, As an AI language, However, it is important to note, whether you like it or not, important to remember that, perhaps, just perhaps, each day continued, hours passed, weeks went by\n",
      "  List 2: I'm sorry, but, I'm sorry but, as an AI, as a Language Model, AI Language Model, language model, However, it is important to, However, it's important, ethical guidelines, just an AI, within my programming, in conclusion, shivers and whispers, audible pop, rivulets of, half-lidded eyes, arousal pooling in her belly, take your pleasure, fiddles with the hem of her skirt, kiss-bruised lips, a bruising kiss, despite herself, yours to take, with reckless abandon, knuckles turning white, grins wickedly, pupils blown wide with pleasure, tongue darts out, grasps your chin and forces you to meet her gaze, bites your ear, nails raking angry red lines down your back, her cheeks flaming, cheeks hollowing, stars burst behind her eyes, inner walls clenching around nothing, puckered hole, her wet heat, she whimpers, biting her lip, dusky nipples, slick folds, still lodged deep inside her, heart, body and soul belong to you, the night is still young, ...for now., haze of pleasure, finds solace in, reveling in the satisfaction, a delicate dance, wet flesh, the ball is in your court, little did he know, little did she know, little did they know, breathless and eager, whispering words of passion, was soft and gentle, shivers down, her sex, sent shockwaves, sent shock waves, in a rhythm, exhausted and spent, life would never be the same again, like an electric shock, threatens to consume, for what seemed like an eternity, as an AI, as a Language Model, AI Language Model, language model, However, it is important to, ethical guidelines, within my programming, in conclusion, couldn't help but, can't resist, I'm sorry, but, I'm sorry but, important to remember that, whether you like it or not, once upon, nestled deep within, an ethereal beauty, for what seemed like an eternity, shivers down, dance of pleasure, sent shockwaves, sent shock waves, in a rhythm, exhausted and spent, life would never be the same again, the night is still young, our shared experiences, bond built on mutual trust, bonds built on mutual trust, the ball is in your court, little did he know, little did she know, little did they know, a pregnant silence, beats like a drum, as an ai, perhaps, just perhaps, what felt like an eternity, maybe, just maybe, the world narrows, breathless and eager, each day continued, hours passed, weeks went by, thanks for reading, thank you for reading, thanks for sharing, thank you for sharing, thanks for posting, thank you for posting, good story, great story, to be continued, end of session, end of story, end of rp, end of roleplay, end of chat, end of chapter\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/02 22:44:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåü Initializing merging and sampling process...\n",
      "\n",
      "üìÇ Loading and sampling dataset jondurbin/airoboros-3.2 with file pattern: processed_chunks/jondurbin_airoboros-3.2_chunk_*.parquet\n",
      "üîÄ Shuffling dataset to randomize row order...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/02 22:44:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Initial row count for jondurbin/airoboros-3.2: 53310\n",
      "üìâ Row count after sampling jondurbin/airoboros-3.2: 53310\n",
      "\n",
      "üìÇ Loading and sampling dataset LLM-Experiments/Steel-Med with file pattern: processed_chunks/LLM-Experiments_Steel-Med_chunk_*.parquet\n",
      "üîÄ Shuffling dataset to randomize row order...\n",
      "üî¢ Initial row count for LLM-Experiments/Steel-Med: 77827\n",
      "üìâ Row count after sampling LLM-Experiments/Steel-Med: 77827\n",
      "\n",
      "üìÇ Loading and sampling dataset mpasila/no-robots-sharegpt-edit with file pattern: processed_chunks/mpasila_no-robots-sharegpt-edit_chunk_*.parquet\n",
      "üîÄ Shuffling dataset to randomize row order...\n",
      "üî¢ Initial row count for mpasila/no-robots-sharegpt-edit: 9765\n",
      "üìâ Row count after sampling mpasila/no-robots-sharegpt-edit: 9765\n",
      "\n",
      "üìÇ Loading and sampling dataset nothingiisreal/Kalomaze-Opus-Instruct-25k-filtered with file pattern: processed_chunks/nothingiisreal_Kalomaze-Opus-Instruct-25k-filtered_chunk_*.parquet\n",
      "üîÄ Shuffling dataset to randomize row order...\n",
      "üî¢ Initial row count for nothingiisreal/Kalomaze-Opus-Instruct-25k-filtered: 44544\n",
      "üìâ Row count after sampling nothingiisreal/Kalomaze-Opus-Instruct-25k-filtered: 44544\n",
      "\n",
      "üìÇ Loading and sampling dataset mrfakename/Pure-Dove-ShareGPT with file pattern: processed_chunks/mrfakename_Pure-Dove-ShareGPT_chunk_*.parquet\n",
      "üîÄ Shuffling dataset to randomize row order...\n",
      "üî¢ Initial row count for mrfakename/Pure-Dove-ShareGPT: 3319\n",
      "üìâ Row count after sampling mrfakename/Pure-Dove-ShareGPT: 3319\n",
      "\n",
      "üìÇ Loading and sampling dataset Undi95/Capybara-ShareGPT with file pattern: processed_chunks/Undi95_Capybara-ShareGPT_chunk_*.parquet\n",
      "üîÄ Shuffling dataset to randomize row order...\n",
      "üî¢ Initial row count for Undi95/Capybara-ShareGPT: 12952\n",
      "üìâ Row count after sampling Undi95/Capybara-ShareGPT: 12952\n",
      "\n",
      "üìÇ Loading and sampling dataset PJMixers/grimulkan_theory-of-mind-ShareGPT with file pattern: processed_chunks/PJMixers_grimulkan_theory-of-mind-ShareGPT_chunk_*.parquet\n",
      "üîÄ Shuffling dataset to randomize row order...\n",
      "üî¢ Initial row count for PJMixers/grimulkan_theory-of-mind-ShareGPT: 533\n",
      "üìâ Row count after sampling PJMixers/grimulkan_theory-of-mind-ShareGPT: 533\n",
      "\n",
      "üìÇ Loading and sampling dataset PJMixers/grimulkan_physical-reasoning-ShareGPT with file pattern: processed_chunks/PJMixers_grimulkan_physical-reasoning-ShareGPT_chunk_*.parquet\n",
      "üîÄ Shuffling dataset to randomize row order...\n",
      "üî¢ Initial row count for PJMixers/grimulkan_physical-reasoning-ShareGPT: 895\n",
      "üìâ Row count after sampling PJMixers/grimulkan_physical-reasoning-ShareGPT: 895\n",
      "\n",
      "üìÇ Loading and sampling dataset TheSkullery/WizardLM_evol_instruct_v2_Filtered_Fuzzy_Dedup_ShareGPT with file pattern: processed_chunks/TheSkullery_WizardLM_evol_instruct_v2_Filtered_Fuzzy_Dedup_ShareGPT_chunk_*.parquet\n",
      "üîÄ Shuffling dataset to randomize row order...\n",
      "üî¢ Initial row count for TheSkullery/WizardLM_evol_instruct_v2_Filtered_Fuzzy_Dedup_ShareGPT: 117485\n",
      "üìâ Row count after sampling TheSkullery/WizardLM_evol_instruct_v2_Filtered_Fuzzy_Dedup_ShareGPT: 58913\n",
      "\n",
      "üìÇ Loading and sampling dataset Undi95/andrijdavid_roleplay-conversation-sharegpt with file pattern: processed_chunks/Undi95_andrijdavid_roleplay-conversation-sharegpt_chunk_*.parquet\n",
      "üîÄ Shuffling dataset to randomize row order...\n",
      "üî¢ Initial row count for Undi95/andrijdavid_roleplay-conversation-sharegpt: 9507\n",
      "üìâ Row count after sampling Undi95/andrijdavid_roleplay-conversation-sharegpt: 9507\n",
      "\n",
      "üìÇ Loading and sampling dataset TheSkullery/Gryphe-Opus-WritingPrompts-merged with file pattern: processed_chunks/TheSkullery_Gryphe-Opus-WritingPrompts-merged_chunk_*.parquet\n",
      "üîÄ Shuffling dataset to randomize row order...\n",
      "üî¢ Initial row count for TheSkullery/Gryphe-Opus-WritingPrompts-merged: 4841\n",
      "üìâ Row count after sampling TheSkullery/Gryphe-Opus-WritingPrompts-merged: 4841\n",
      "\n",
      "üìÇ Loading and sampling dataset mpasila/LimaRP-PIPPA-freedom-rp-Mix-8K with file pattern: processed_chunks/mpasila_LimaRP-PIPPA-freedom-rp-Mix-8K_chunk_*.parquet\n",
      "üîÄ Shuffling dataset to randomize row order...\n",
      "üî¢ Initial row count for mpasila/LimaRP-PIPPA-freedom-rp-Mix-8K: 3187\n",
      "üìâ Row count after sampling mpasila/LimaRP-PIPPA-freedom-rp-Mix-8K: 3187\n",
      "\n",
      "üìÇ Loading and sampling dataset Alignment-Lab-AI/RPGuild-sharegpt-filtered with file pattern: processed_chunks/Alignment-Lab-AI_RPGuild-sharegpt-filtered_chunk_*.parquet\n",
      "üîÄ Shuffling dataset to randomize row order...\n",
      "üî¢ Initial row count for Alignment-Lab-AI/RPGuild-sharegpt-filtered: 21688\n",
      "üìâ Row count after sampling Alignment-Lab-AI/RPGuild-sharegpt-filtered: 21688\n",
      "\n",
      "üìÇ Loading and sampling dataset Steelskull/SLOP-MEGA with file pattern: processed_chunks/Steelskull_SLOP-MEGA_chunk_*.parquet\n",
      "üîÄ Shuffling dataset to randomize row order...\n",
      "üî¢ Initial row count for Steelskull/SLOP-MEGA: 5044\n",
      "üìâ Row count after sampling Steelskull/SLOP-MEGA: 5044\n",
      "\n",
      "üîó Unioning all sampled dataframes...\n",
      "\n",
      "\n",
      "üìã Files successfully loaded. Inspecting schema...\n",
      "|-- conversations: array<struct<from:string,value:string>> (nullable = True)\n",
      "|-- system: string (nullable = True)\n",
      "|-- tools: string (nullable = True)\n",
      "|-- origin: string (nullable = True)\n",
      "|-- script_version: string (nullable = True)\n",
      "|-- human_token_count: bigint (nullable = True)\n",
      "|-- gpt_token_count: bigint (nullable = True)\n",
      "\n",
      "üîÄ Shuffling final dataset to randomize row order...\n",
      "\n",
      "‚úÖ Before deduplication row count: 306325\n",
      "‚ùó Schema does not match expected format. Adaptation might be required.\n",
      "Adding embeddings to DataFrame...\n",
      "Converting DataFrame to Pandas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/02 22:45:33 WARN DAGScheduler: Broadcasting large task binary with size 1513.0 KiB\n",
      "24/08/02 22:45:34 WARN DAGScheduler: Broadcasting large task binary with size 1514.3 KiB\n",
      "24/08/02 22:45:36 WARN DAGScheduler: Broadcasting large task binary with size 1519.2 KiB\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/opt/conda/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Embeddings: 100%|##########| 306309/306309 [00:00<00:00, 4493321.90it/s]\n",
      "Number of embeddings: 306309\n",
      "Calculating cosine similarity matrix...\n",
      "Number of embeddings: 306,309, Embedding dimension: 768\n",
      "Total System Memory: 125.72 GB\n",
      "Available Memory (75%): 51.30 GB\n",
      "Single Embedding Size: 3.00 KB\n",
      "Maximum matrix size that can fit in memory: 117347 x 117347\n",
      "Total number of embeddings in dataset: 306,309\n",
      "Calculated chunk size: 117,347\n",
      "Estimated memory usage per chunk: 51.30 GB\n",
      "Creating memory-mapped file for embeddings...\n",
      "Total number of chunks to process: 6\n",
      "Processing chunks:   0%|          | 0/6 [00:00<?, ?it/s]Calculated sub-chunk size: 116744\n",
      "Estimated memory usage per sub-chunk: 50.77 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import socket\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, Manager, Process, Value\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import fasttext\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ollama\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from langdetect import detect, LangDetectException\n",
    "from matplotlib import cm, colors\n",
    "from ollama import Client\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, to_json, monotonically_increasing_id, rand, row_number, hash, pandas_udf, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StructType\n",
    "from pyspark.sql.window import Window\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.sparse import lil_matrix, save_npz, csr_matrix, load_npz\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from umap import UMAP\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, filename=\"logfile.log\"):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(filename, \"a\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "        self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        self.log.flush()\n",
    "\n",
    "sys.stdout = Logger(\"output.log\")\n",
    "sys.stderr = Logger(\"output.log\")\n",
    "\n",
    "def load_config(file_path):\n",
    "    \"\"\"Load a YAML configuration file.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def create_default_configs(config_dir):\n",
    "    \"\"\"Create default configuration files if they do not exist.\"\"\"\n",
    "    os.makedirs(config_dir, exist_ok=True)\n",
    "    \n",
    "    main_config_path = os.path.join(config_dir, 'main_config.yml')\n",
    "    user_config_path = os.path.join(config_dir, 'user_config.yml')\n",
    "    dataset_config_path = os.path.join(config_dir, 'dataset_config.yml')\n",
    "    phrase_lists_path = os.path.join(config_dir, 'phrase_lists.yml')\n",
    "    \n",
    "    default_main = {\n",
    "        'chunk_size': 10000,\n",
    "        'core_count': 'max(2, multiprocessing.cpu_count() - 4)',\n",
    "        'processed_chunks_dir': './processed_chunks',\n",
    "        'script_version': 'DMKv3.5',\n",
    "        'rewrite_sys_prompt': False,\n",
    "        'sys_prompt_min_token_length': 50\n",
    "    }\n",
    "    default_user = {\n",
    "        'Dataset_Name': 'Default_Dataset',\n",
    "        'User': 'Default_User'\n",
    "    }\n",
    "    default_dataset = {\n",
    "        'key_name': 'conversations',\n",
    "        'acceptable_languages': ['en'],\n",
    "        'filtered_data': ['function-call', 'function-response', 'assistant'],\n",
    "        'tokenizer_used': 'NousResearch/Meta-Llama-3-70B-Instruct',\n",
    "        'datasets': [\n",
    "            {\n",
    "                'name': 'default_dataset_name',\n",
    "                'use_percentage': 100,\n",
    "                'phrase_list_key': 1\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    default_phrases = {\n",
    "        'phrase_lists': {\n",
    "            1: [\n",
    "                \"example phrase 1\",\n",
    "                \"example phrase 2\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_files = {\n",
    "        main_config_path: default_main,\n",
    "        user_config_path: default_user,\n",
    "        dataset_config_path: default_dataset,\n",
    "        phrase_lists_path: default_phrases\n",
    "    }\n",
    "    \n",
    "    for path, default_config in config_files.items():\n",
    "        if not os.path.exists(path):\n",
    "            with open(path, 'w') as file:\n",
    "                yaml.safe_dump(default_config, file)\n",
    "    \n",
    "    print(\"INFO: Checked and/or created default configuration files in\", config_dir)\n",
    "\n",
    "def update_config_file(config_path, new_options):\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "    else:\n",
    "        config = {}\n",
    "    \n",
    "    updated = False\n",
    "    for key, value in new_options.items():\n",
    "        if key not in config:\n",
    "            config[key] = value\n",
    "            updated = True\n",
    "    \n",
    "    if updated:\n",
    "        with open(config_path, 'w') as file:\n",
    "            yaml.safe_dump(config, file)\n",
    "        print(f\"Updated configuration file: {config_path}\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "config_directory = './config_files'\n",
    "create_default_configs(config_directory)\n",
    "\n",
    "main_config_path = os.path.join(config_directory, 'main_config.yml')\n",
    "new_options = {\n",
    "    'rewrite_sys_prompt': False,\n",
    "    'sys_prompt_min_token_length': 50\n",
    "}\n",
    "main_config = update_config_file(main_config_path, new_options)\n",
    "\n",
    "user_config = load_config(os.path.join(config_directory, 'user_config.yml'))\n",
    "dataset_config = load_config(os.path.join(config_directory, 'dataset_config.yml'))\n",
    "phrase_lists_config = load_config(os.path.join(config_directory, 'phrase_lists.yml'))\n",
    "\n",
    "key_name = dataset_config['key_name']\n",
    "chunk_size = main_config['chunk_size']\n",
    "core = main_config['core_count']\n",
    "processed_chunks_dir = main_config['processed_chunks_dir']\n",
    "script_version = main_config['script_version']\n",
    "User = user_config['User']\n",
    "Dataset_Name = user_config['Dataset_Name']\n",
    "rewrite_sys_prompt = main_config['rewrite_sys_prompt']\n",
    "sys_prompt_min_token_length = main_config['sys_prompt_min_token_length']\n",
    "\n",
    "core_count = max(2, multiprocessing.cpu_count() - 4)\n",
    "\n",
    "datasets = dataset_config['datasets']\n",
    "filtered_data = dataset_config['filtered_data']\n",
    "phrase_lists = phrase_lists_config['phrase_lists']\n",
    "acceptable_languages = dataset_config['acceptable_languages']\n",
    "tokenizer_used = dataset_config['tokenizer_used']\n",
    "\n",
    "os.makedirs(processed_chunks_dir, exist_ok=True)\n",
    "\n",
    "def ensure_model_downloaded(model_path, download_url):\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    if not os.path.isfile(model_path):\n",
    "        print(f\"Model not found at {model_path}. Downloading from {download_url}...\")\n",
    "        response = requests.get(download_url)\n",
    "        with open(model_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Download complete.\")\n",
    "\n",
    "model_path = './model/lid.176.bin'\n",
    "download_url = 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin'\n",
    "ensure_model_downloaded(model_path, download_url)\n",
    "\n",
    "language_model = fasttext.load_model(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_used)\n",
    "\n",
    "print(f\"Full_Configuration:\")\n",
    "print(f\"\\nUser Configuration:\")\n",
    "print(f\"User: {User}\")\n",
    "print(f\"Merged_Dataset_name: {Dataset_Name}\")\n",
    "print(f\"\\nSystem Configuration:\")\n",
    "print(f\"  Key Name: {key_name}\")\n",
    "print(f\"  CPU Threads Used: {core_count}\")\n",
    "print(f\"  Chunk Size: {chunk_size}\")\n",
    "print(f\"  Processed Chunks Directory: {processed_chunks_dir}\")\n",
    "print(f\"  Script Version: {script_version}\")\n",
    "print(f\"  Rewrite System Prompt: {rewrite_sys_prompt}\")\n",
    "print(f\"  System Prompt Minimum Token Length: {sys_prompt_min_token_length}\")\n",
    "\n",
    "def format_dataset_info(datasets):\n",
    "    info = \"Configured Datasets:\\n\"\n",
    "    for dataset in datasets:\n",
    "        info += f\"  - Name: {dataset['name']}\\n\"\n",
    "        info += f\"    Use Percentage: {dataset['use_percentage']}%\\n\"\n",
    "        info += f\"    Phrase List Key: {dataset['phrase_list_key']}\\n\"\n",
    "    return info\n",
    "\n",
    "print(format_dataset_info(datasets))\n",
    "print(f\"\\nFiltered Data Types: {', '.join(filtered_data)}\")\n",
    "#print(f\"\\nTokenizer Used: {tokenizer}\")\n",
    "print(f\"\\nacceptable_languages: {acceptable_languages}\")\n",
    "\n",
    "print(\"\\nPhrase Lists:\")\n",
    "for key, phrases in phrase_lists.items():\n",
    "    print(f\"  List {key}: {', '.join(phrases)}\")\n",
    "\n",
    "def load_mappings_from_config(dataset_config, phrase_lists_config):\n",
    "    dataset_names = [ds['name'] for ds in dataset_config['datasets']]\n",
    "    dataset_phrase_keys = {ds['name']: ds['phrase_list_key'] for ds in dataset_config['datasets']}\n",
    "    dataset_use_percentage = {ds['name']: ds['use_percentage'] for ds in dataset_config['datasets']}\n",
    "    phrase_lists = phrase_lists_config['phrase_lists']\n",
    "    filtered_data = dataset_config['filtered_data']\n",
    "    acceptable_languages = dataset_config['acceptable_languages']\n",
    "    return dataset_names, phrase_lists, dataset_phrase_keys, filtered_data, dataset_use_percentage, acceptable_languages\n",
    "\n",
    "dataset_names, phrase_lists, dataset_phrase_keys, filtered_data, dataset_use_percentage, acceptable_languages = load_mappings_from_config(dataset_config, phrase_lists_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b3dbaf-149a-46a5-bcf7-a82212297d10",
   "metadata": {
    "id": "d33c0306-cb82-41fd-8600-06d3a45c7beb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import socket\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, Manager, Process, Value\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import fasttext\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ollama\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from langdetect import detect, LangDetectException\n",
    "from matplotlib import cm, colors\n",
    "from ollama import Client\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, to_json, monotonically_increasing_id, rand, row_number, hash, pandas_udf, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StructType\n",
    "from pyspark.sql.window import Window\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.sparse import lil_matrix, save_npz, csr_matrix, load_npz\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from umap import UMAP\n",
    "\n",
    "\n",
    "max_retries = 3\n",
    "retry_delay = 1\n",
    "num_request_workers = 20\n",
    "ollama_host = 'http://0.0.0.0:1143'\n",
    "ollama_model = '[model name]'\n",
    "\n",
    "def initialize_token_distribution(start, end, step):\n",
    "    return {f\"{i}-{min(i+step-1, end)}\": 0 for i in range(start, end, step)}\n",
    "\n",
    "def update_token_distribution(token_distribution, token_count):\n",
    "    for key in token_distribution:\n",
    "        start, end = map(int, key.split('-'))\n",
    "        if start <= token_count <= end:\n",
    "            token_distribution[key] += 1\n",
    "            break\n",
    "\n",
    "def create_regex_pattern(dataset_name, dataset_phrase_keys, phrase_lists):\n",
    "    phrase_key = dataset_phrase_keys.get(dataset_name)\n",
    "    if not phrase_key:\n",
    "        return None\n",
    "    phrases = phrase_lists.get(phrase_key, [])\n",
    "    return re.compile('|'.join([re.escape(phrase).replace('\\\\ ', '\\\\s+') for phrase in phrases]), re.IGNORECASE)\n",
    "\n",
    "def sanitize_text(text):\n",
    "    return text.replace('\\n', ' ').replace('\\r', ' ').strip()\n",
    "\n",
    "def filter_conversation(conversations, regex_pattern, filtered_data, acceptable_languages):\n",
    "    for msg in conversations:\n",
    "        content = sanitize_text(msg.get('value', ''))\n",
    "        if msg.get(\"from\") == \"human\":\n",
    "            try:\n",
    "                if detect(content) not in acceptable_languages:\n",
    "                    return None, True\n",
    "            except LangDetectException:\n",
    "                pass\n",
    "        if msg.get(\"from\") in filtered_data or (regex_pattern and regex_pattern.search(content)):\n",
    "            return None, True\n",
    "    return conversations, False\n",
    "\n",
    "def transform_record(record, dataset_name, script_version, tokenizer):\n",
    "    try:\n",
    "        final_conversations = []\n",
    "        system = record.get('system', '')\n",
    "        tools = record.get('tools', '')\n",
    "        human_token_count = gpt_token_count = 0\n",
    "        token_distribution = initialize_token_distribution(0, 32000, 16)\n",
    "\n",
    "        for item in record.get('conversations', []):\n",
    "            if item['from'] not in ['system', 'tools']:\n",
    "                token_count = len(tokenizer.tokenize(item['value']))\n",
    "                if item['from'] == 'human':\n",
    "                    human_token_count += token_count\n",
    "                    if len(final_conversations) % 2 == 0:\n",
    "                        final_conversations.append(item)\n",
    "                elif item['from'] == 'gpt':\n",
    "                    gpt_token_count += token_count\n",
    "                    if len(final_conversations) % 2 != 0:\n",
    "                        final_conversations.append(item)\n",
    "                update_token_distribution(token_distribution, token_count)\n",
    "\n",
    "        return {\n",
    "            'conversations': final_conversations,\n",
    "            'system': system,\n",
    "            'tools': tools,\n",
    "            'origin': dataset_name,\n",
    "            'script_version': script_version,\n",
    "            'human_token_count': human_token_count,\n",
    "            'gpt_token_count': gpt_token_count,\n",
    "            'token_distribution': token_distribution,\n",
    "        }, False\n",
    "    except Exception as e:\n",
    "        print(f\"Error in transform_record: {e}\")\n",
    "        return None, True\n",
    "\n",
    "def write_chunk_to_file(chunk_data, dataset_name, chunk_index, processed_chunks_dir):\n",
    "    for record in chunk_data:\n",
    "        record['token_distribution'] = {str(k): v for k, v in record['token_distribution'].items()}\n",
    "    \n",
    "    filename = f\"{processed_chunks_dir}/{dataset_name.replace('/', '_')}_chunk_{chunk_index}.parquet\"\n",
    "    table = pa.Table.from_pandas(pd.DataFrame(chunk_data))\n",
    "    pq.write_table(table, filename)\n",
    "\n",
    "def request_worker(input_queue, output_dict, worker_id):\n",
    "    client = Client(host=ollama_host)\n",
    "    while True:\n",
    "        try:\n",
    "            prompt_id, prompt = input_queue.get()\n",
    "            if prompt_id is None:  # Sentinel value to stop the worker\n",
    "                break\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    response = client.generate(ollama_model, prompt)\n",
    "                    output_dict[prompt_id] = response['response'].strip()\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt == max_retries - 1:\n",
    "                        print(f\"Error in request after {max_retries} attempts: {e}\")\n",
    "                        output_dict[prompt_id] = None\n",
    "                    else:\n",
    "                        time.sleep(retry_delay)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error in request worker {worker_id}: {e}\")\n",
    "        finally:\n",
    "            input_queue.task_done()\n",
    "\n",
    "def rewrite_system_prompt(conversations, tokenizer, input_queue, output_dict, prompt_id):\n",
    "    context = \"\\n\".join([f\"{msg['from']}: {msg['value']}\" for msg in conversations[:5]])\n",
    "    prompt = f\"Based on the following conversation, write a detailed system prompt that sets the context and guidelines for an AI model. This model can be used for various purposes, including role-playing (RP), erotic role-playing (ERP), intelligence tasks, general AI assistance, code generation, and more. Ensure the system prompt is comprehensive and specific to the task at hand. Only output the system prompt based on the context:\\n\\n{context}\\n\\nSystem prompt:\"\n",
    "\n",
    "    input_queue.put((prompt_id, prompt))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while prompt_id not in output_dict and time.time() - start_time < 60:\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    new_system_prompt = output_dict.get(prompt_id)\n",
    "    if new_system_prompt and len(tokenizer.tokenize(new_system_prompt)) >= sys_prompt_min_token_length:\n",
    "        return new_system_prompt\n",
    "    else:\n",
    "        print(\"Timeout or invalid result while waiting for system prompt rewrite. Using original system prompt.\")\n",
    "        return None\n",
    "\n",
    "def process_chunk(args):\n",
    "    dataset_name, chunk_index, regex_pattern, filtered_data, data_chunk, processed_chunks_dir, script_version, acceptable_languages, tokenizer, input_queue, output_dict = args\n",
    "    results = []\n",
    "    removed_count = 0\n",
    "    token_distribution = initialize_token_distribution(0, 32000, 16)\n",
    "    rewrites_count = 0\n",
    "    \n",
    "    for record in data_chunk:\n",
    "        filtered_conversations, was_removed = filter_conversation(\n",
    "            record['conversations'], regex_pattern, filtered_data, acceptable_languages\n",
    "        )\n",
    "        if was_removed:\n",
    "            removed_count += 1\n",
    "            continue\n",
    "\n",
    "        transformed_record, transform_error = transform_record(record, dataset_name, script_version, tokenizer)\n",
    "        if transformed_record:\n",
    "            current_sys_prompt_length = len(tokenizer.tokenize(transformed_record['system'] if 'system' in transformed_record else \"\"))\n",
    "            if rewrite_sys_prompt and current_sys_prompt_length < sys_prompt_min_token_length:\n",
    "                prompt_id = f\"{chunk_index}_{len(results)}\"\n",
    "                new_system_prompt = rewrite_system_prompt(transformed_record['conversations'], tokenizer, input_queue, output_dict, prompt_id)\n",
    "                if new_system_prompt:\n",
    "                    transformed_record['system'] = new_system_prompt\n",
    "                    rewrites_count += 1\n",
    "            \n",
    "            results.append(transformed_record)\n",
    "            for key in transformed_record['token_distribution']:\n",
    "                token_distribution[key] += transformed_record['token_distribution'][key]\n",
    "        if transform_error:\n",
    "            removed_count += 1\n",
    "\n",
    "    if results:\n",
    "        write_chunk_to_file(results, dataset_name, chunk_index, processed_chunks_dir)\n",
    "        \n",
    "    return {\n",
    "        'chunk_index': chunk_index, \n",
    "        'results': len(results), \n",
    "        'removed_count': removed_count,\n",
    "        'token_distribution': token_distribution,\n",
    "        'rewrites_count': rewrites_count\n",
    "    }\n",
    "\n",
    "def load_data_chunk_hf(dataset_name, split='train', chunk_size=None):\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    chunk = []\n",
    "    for idx, example in enumerate(dataset):\n",
    "        if isinstance(example, str):\n",
    "            example = json.loads(example)\n",
    "\n",
    "        conversations = example.get('conversations', [])\n",
    "        system = example.get('system', '')\n",
    "        tools = example.get('tools', '')\n",
    "\n",
    "        for item in conversations:\n",
    "            if item['from'] == 'system':\n",
    "                system = item['value']\n",
    "            elif item['from'] == 'tools':\n",
    "                tools = item['value']\n",
    "        \n",
    "        record = {\n",
    "            'conversations': [conv for conv in conversations if conv['from'] not in ['system', 'tools']],\n",
    "            'system': system,\n",
    "            'tools': tools\n",
    "        }\n",
    "\n",
    "        chunk.append(record)\n",
    "        if chunk_size and len(chunk) == chunk_size:\n",
    "            yield chunk\n",
    "            chunk = []\n",
    "\n",
    "    if chunk:\n",
    "        yield chunk\n",
    "\n",
    "def process_datasets_in_chunks(dataset_names, chunk_size, filtered_data, core_count, dataset_phrase_keys, phrase_lists, processed_chunks_dir, script_version, acceptable_languages, tokenizer):\n",
    "    dataset_summary = {}\n",
    "    manager = Manager()\n",
    "    input_queue = manager.Queue()\n",
    "    output_dict = manager.dict()\n",
    "\n",
    "    workers = []\n",
    "    for i in range(num_request_workers):\n",
    "        p = Process(target=request_worker, args=(input_queue, output_dict, i))\n",
    "        p.start()\n",
    "        workers.append(p)\n",
    "\n",
    "    with Pool(processes=core_count) as pool:\n",
    "        for dataset_name in dataset_names:\n",
    "            dataset_summary[dataset_name] = {\n",
    "                'processed': 0,\n",
    "                'removed': 0,\n",
    "                'token_distribution': initialize_token_distribution(0, 32000, 16),\n",
    "                'rewrites_count': 0\n",
    "            }\n",
    "            regex_pattern = create_regex_pattern(dataset_name, dataset_phrase_keys, phrase_lists)\n",
    "            \n",
    "            tasks = []\n",
    "            total_records = sum(1 for _ in load_data_chunk_hf(dataset_name, chunk_size=chunk_size))\n",
    "            \n",
    "            with tqdm(total=total_records, desc=f\"Processing {dataset_name}\") as pbar:\n",
    "                for chunk_index, data_chunk in enumerate(load_data_chunk_hf(dataset_name, chunk_size=chunk_size)):\n",
    "                    args = (dataset_name, chunk_index, regex_pattern, filtered_data, data_chunk, processed_chunks_dir, script_version, acceptable_languages, tokenizer, input_queue, output_dict)\n",
    "                    task = pool.apply_async(process_chunk, (args,))\n",
    "                    tasks.append((dataset_name, task))\n",
    "\n",
    "                for dataset_name, task in tasks:\n",
    "                    result = task.get()\n",
    "                    dataset_summary[dataset_name]['processed'] += result['results']\n",
    "                    dataset_summary[dataset_name]['removed'] += result['removed_count']\n",
    "                    dataset_summary[dataset_name]['rewrites_count'] += result['rewrites_count']\n",
    "                    for key in result['token_distribution']:\n",
    "                        dataset_summary[dataset_name]['token_distribution'][key] += result['token_distribution'][key]\n",
    "                    pbar.update(result['results'] + result['removed_count'])\n",
    "\n",
    "    for _ in range(num_request_workers):\n",
    "        input_queue.put((None, None))\n",
    "    for p in workers:\n",
    "        p.join()\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    for dataset_name in dataset_summary:\n",
    "        dataset_summary[dataset_name]['token_distribution'] = {k: v for k, v in dataset_summary[dataset_name]['token_distribution'].items() if v > 0}\n",
    "\n",
    "    print(\"\\nDataset Summary (Processed / Removed / Rewrites):\")\n",
    "    for name, summary in dataset_summary.items():\n",
    "        print(f\"{name}: {summary['processed']} / {summary['removed']} / {summary['rewrites_count']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting dataset processing...\")\n",
    "    \n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(f\"Rewrite System Prompt: {rewrite_sys_prompt}\")\n",
    "    print(f\"System Prompt Minimum Token Length: {sys_prompt_min_token_length}\")\n",
    "    print(f\"Chunk Size: {chunk_size}\")\n",
    "    print(f\"Core Count: {core_count}\")\n",
    "    print(f\"Processed Chunks Directory: {processed_chunks_dir}\")\n",
    "    print(f\"Script Version: {script_version}\")\n",
    "    print(f\"Ollama Host: {ollama_host}\")\n",
    "    print(f\"Ollama Model: {ollama_model}\")\n",
    "    \n",
    "    try:\n",
    "        client = Client(host=ollama_host)\n",
    "        response = client.generate(ollama_model, 'Test connection')\n",
    "        print(\"Successfully connected to the Ollama service.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Unable to connect to the Ollama service: {e}\")\n",
    "        print(\"The script will continue without rewriting system prompts.\")\n",
    "        rewrite_sys_prompt = False\n",
    "    \n",
    "    process_datasets_in_chunks(\n",
    "        dataset_names,\n",
    "        chunk_size,\n",
    "        filtered_data,\n",
    "        core_count,\n",
    "        dataset_phrase_keys,\n",
    "        phrase_lists,\n",
    "        processed_chunks_dir,\n",
    "        script_version,\n",
    "        acceptable_languages,\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    print(\"\\nDataset processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80327ba-4ee4-4594-bfd0-39827ad28128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deduplicate and merge\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import socket\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, Manager, Process, Value\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import fasttext\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ollama\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from langdetect import detect, LangDetectException\n",
    "from matplotlib import cm, colors\n",
    "from ollama import Client\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, to_json, monotonically_increasing_id, rand, row_number, hash, pandas_udf, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StructType\n",
    "from pyspark.sql.window import Window\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.sparse import lil_matrix, save_npz, csr_matrix, load_npz\n",
    "from scipy.sparse import eye\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from umap import UMAP\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avsolatorio/GIST-Embedding-v0\")\n",
    "model = AutoModel.from_pretrained(\"avsolatorio/GIST-Embedding-v0\")\n",
    "\n",
    "# Checking if a GPU is available and using it\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "def initialize_spark_session(core_count):\n",
    "    \"\"\"\n",
    "    Initialize and return a Spark session with specified core count settings\n",
    "    and an increased driver max result size.\n",
    "    \"\"\"\n",
    "    spark_memory = \"15g\"  # Adjust based on your system's resources\n",
    "    driver_max_result_size=\"2g\"\n",
    "    spark = SparkSession.builder\\\n",
    "        .appName(\"Process and Merge Conversations\")\\\n",
    "        .master(f\"local[{core_count}]\")\\\n",
    "        .config(\"spark.driver.memory\", spark_memory)\\\n",
    "        .config(\"spark.executor.memory\", spark_memory)\\\n",
    "        .config(\"spark.executor.cores\", str(core_count))\\\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(core_count * 2))\\\n",
    "        .config(\"spark.driver.maxResultSize\", driver_max_result_size)\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "    \n",
    "def adapt_schema(sdf):\n",
    "    \"\"\"\n",
    "    Check and adapt the schema of the Spark DataFrame if necessary.\n",
    "    \"\"\"\n",
    "    expected_columns = [\"conversations\", \"system\", \"tools\", \"origin\", \"script_version\", \"human_token_count\", \"gpt_token_count\", \"token_distribution\", \"processing_time_ms\"]\n",
    "    if set(sdf.columns) == set(expected_columns):\n",
    "        print(\"‚úîÔ∏è Schema matches the expected format. No adaptation needed.\")\n",
    "    else:\n",
    "        print(\"‚ùó Schema does not match expected format. Adaptation might be required.\")\n",
    "\n",
    "    return sdf\n",
    "\n",
    "def perform_reduction(method, embeddings):\n",
    "    return method.fit_transform(embeddings)\n",
    "\n",
    "def get_available_memory():\n",
    "    return psutil.virtual_memory().available\n",
    "\n",
    "def calculate_chunk_size(n, embedding_dim, dtype=np.float32):\n",
    "    def bytes_to_human_readable(bytes_value):\n",
    "        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "            if bytes_value < 1024.0:\n",
    "                return f\"{bytes_value:.2f} {unit}\"\n",
    "            bytes_value /= 1024.0\n",
    "\n",
    "    # Calculate memory footprint of one embedding\n",
    "    embedding_size = np.dtype(dtype).itemsize * embedding_dim\n",
    "    \n",
    "    # Get available memory (using 75% of available memory)\n",
    "    total_memory = psutil.virtual_memory().total\n",
    "    available_memory = get_available_memory() * 0.5\n",
    "    \n",
    "    print(f\"Total System Memory: {bytes_to_human_readable(total_memory)}\")\n",
    "    print(f\"Available Memory (75%): {bytes_to_human_readable(available_memory)}\")\n",
    "    print(f\"Single Embedding Size: {bytes_to_human_readable(embedding_size)}\")\n",
    "    \n",
    "    # Calculate the maximum square matrix size that can fit in memory\n",
    "    max_matrix_size = int(np.sqrt(available_memory / np.dtype(dtype).itemsize))\n",
    "    \n",
    "    # Ensure chunk size is at least 1 and no larger than the dataset or max_matrix_size\n",
    "    chunk_size = min(n, max_matrix_size)\n",
    "    \n",
    "    print(f\"Maximum matrix size that can fit in memory: {max_matrix_size} x {max_matrix_size}\")\n",
    "    print(f\"Total number of embeddings in dataset: {n:,}\")\n",
    "    print(f\"Calculated chunk size: {chunk_size:,}\")\n",
    "    \n",
    "    memory_usage = chunk_size * chunk_size * np.dtype(dtype).itemsize\n",
    "    print(f\"Estimated memory usage per chunk: {bytes_to_human_readable(memory_usage)}\")\n",
    "    \n",
    "    return chunk_size\n",
    "\n",
    "def calculate_sub_chunk_size(batch_size, embedding_dim, dtype=np.float32):\n",
    "    # Get available memory (using 75% of available memory to be safe)\n",
    "    available_memory = psutil.virtual_memory().available * 0.25\n",
    "    \n",
    "    # Calculate memory footprint of one sub-chunk\n",
    "    element_size = np.dtype(dtype).itemsize\n",
    "    sub_chunk_memory = lambda size: size * size * element_size\n",
    "    \n",
    "    # Binary search to find the largest sub-chunk size that fits in memory\n",
    "    low, high = 1, batch_size\n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        if sub_chunk_memory(mid) <= available_memory:\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid - 1\n",
    "    \n",
    "    sub_chunk_size = high\n",
    "    \n",
    "    print(f\"Calculated sub-chunk size: {sub_chunk_size}\")\n",
    "    print(f\"Estimated memory usage per sub-chunk: {sub_chunk_memory(sub_chunk_size) / (1024**3):.2f} GB\")\n",
    "    \n",
    "    return sub_chunk_size\n",
    "\n",
    "def cosine_similarity(embeddings, core_count, save_path):\n",
    "    # Convert embeddings to numpy array if it's a list\n",
    "    if isinstance(embeddings, list):\n",
    "        embeddings = np.array(embeddings, dtype=np.float32)\n",
    "    \n",
    "    n, embedding_dim = embeddings.shape\n",
    "    print(f\"Number of embeddings: {n:,}, Embedding dimension: {embedding_dim}\")\n",
    "    \n",
    "    chunk_size = calculate_chunk_size(n, embedding_dim, dtype=np.float32)\n",
    "    ensure_directory_exists(save_path)\n",
    "    \n",
    "    mmap_path = os.path.join(save_path, 'embeddings.mmap')\n",
    "    print(\"Creating memory-mapped file for embeddings...\")\n",
    "    mmap_embeddings = np.memmap(mmap_path, dtype=np.float32, mode='w+', shape=embeddings.shape)\n",
    "    mmap_embeddings[:] = embeddings[:]\n",
    "    mmap_embeddings.flush()\n",
    "    \n",
    "    chunks = [(i, j) for i in range(0, n, chunk_size) for j in range(i, n, chunk_size)]\n",
    "    \n",
    "    print(f\"Total number of chunks to process: {len(chunks)}\")\n",
    "    \n",
    "    for i, j in tqdm(chunks, desc=\"Processing chunks\"): #last run stop\n",
    "        end_i = min(i + chunk_size, n)\n",
    "        end_j = min(j + chunk_size, n)\n",
    "        result_path = os.path.join(save_path, f\"similarity_chunk_{i}_{j}.npz\")\n",
    "        \n",
    "        if not os.path.exists(result_path):\n",
    "            try:\n",
    "                calculate_and_save_cosine_similarity(mmap_embeddings, i, end_i, j, end_j, result_path, chunk_size)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing chunk {i},{j}: {str(e)}\")\n",
    "    \n",
    "    # Verify that at least one chunk was processed successfully\n",
    "    if not any(os.path.exists(os.path.join(save_path, f\"similarity_chunk_{i}_{j}.npz\")) for i, j in chunks):\n",
    "        raise ValueError(\"No similarity chunks were successfully created.\")\n",
    "    \n",
    "    del mmap_embeddings\n",
    "    try:\n",
    "        os.unlink(mmap_path)\n",
    "        print(f\"Successfully deleted {mmap_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete {mmap_path}: {str(e)}\")\n",
    "    \n",
    "    print(\"Cosine similarity calculation completed.\")\n",
    "\n",
    "def calculate_and_save_cosine_similarity(mmap_embeddings, start_i, end_i, start_j, end_j, result_path, chunk_size):\n",
    "    try:\n",
    "        batch_i = mmap_embeddings[start_i:end_i]\n",
    "        batch_j = mmap_embeddings[start_j:end_j]\n",
    "        \n",
    "        # Normalize the embeddings\n",
    "        batch_i = batch_i / (np.linalg.norm(batch_i, axis=1, keepdims=True) + 1e-8)\n",
    "        batch_j = batch_j / (np.linalg.norm(batch_j, axis=1, keepdims=True) + 1e-8)\n",
    "        \n",
    "        # Calculate the sub-chunk size\n",
    "        sub_chunk_size = calculate_sub_chunk_size(max(batch_i.shape[0], batch_j.shape[0]), batch_i.shape[1])\n",
    "        \n",
    "        # Calculate cosine similarity in smaller sub-chunks\n",
    "        similarity = lil_matrix((batch_i.shape[0], batch_j.shape[0]), dtype=np.float32)\n",
    "        \n",
    "        for i in range(0, batch_i.shape[0], sub_chunk_size):\n",
    "            for j in range(0, batch_j.shape[0], sub_chunk_size):\n",
    "                sub_i = batch_i[i:i+sub_chunk_size]\n",
    "                sub_j = batch_j[j:j+sub_chunk_size]\n",
    "                sub_sim = np.dot(sub_i, sub_j.T)\n",
    "                similarity[i:i+sub_chunk_size, j:j+sub_chunk_size] = sub_sim\n",
    "        \n",
    "        # Convert to CSR format for efficient storage\n",
    "        similarity = similarity.tocsr()\n",
    "        \n",
    "        # Save as sparse matrix\n",
    "        save_npz(result_path, similarity)\n",
    "        \n",
    "        print(f\"Saved similarity chunk to {result_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculate_and_save_cosine_similarity: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def ensure_directory_exists(directory_path):\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    print(f\"Ensured directory exists: {directory_path}\")\n",
    "\n",
    "def load_complete_similarity_matrix(save_path):\n",
    "    print(\"Starting to load similarity matrix...\")\n",
    "    max_index_i = max_index_j = 0\n",
    "    file_dimensions = {}\n",
    "\n",
    "    # First pass: determine matrix dimensions\n",
    "    for filename in os.listdir(save_path):\n",
    "        if filename.endswith('.npz'):\n",
    "            parts = filename[:-4].split('_')\n",
    "            if len(parts) >= 4 and parts[-3] == 'chunk':\n",
    "                i, j = int(parts[-2]), int(parts[-1])\n",
    "                filepath = os.path.join(save_path, filename)\n",
    "                chunk = load_npz(filepath)\n",
    "                chunk_shape = chunk.shape\n",
    "                file_dimensions[(i, j)] = chunk_shape\n",
    "                max_index_i = max(max_index_i, i + chunk_shape[0])\n",
    "                max_index_j = max(max_index_j, j + chunk_shape[1])\n",
    "\n",
    "    print(f\"Full matrix dimensions: {max_index_i} x {max_index_j}\")\n",
    "    \n",
    "    if max_index_i == 0 or max_index_j == 0:\n",
    "        raise ValueError(\"No valid similarity matrix chunks found.\")\n",
    "    \n",
    "    # Initialize the full matrix\n",
    "    sim_matrix = csr_matrix((max_index_i, max_index_j), dtype=np.float32)\n",
    "\n",
    "    # Second pass: load chunks and fill the matrix\n",
    "    for (i, j), chunk_shape in tqdm(file_dimensions.items(), desc=\"Loading chunks\"):\n",
    "        filepath = os.path.join(save_path, f\"similarity_chunk_{i}_{j}.npz\")\n",
    "        chunk = load_npz(filepath)\n",
    "        sim_matrix[i:i+chunk_shape[0], j:j+chunk_shape[1]] = chunk\n",
    "\n",
    "    print(\"Similarity matrix loaded successfully.\")\n",
    "    print(f\"Final similarity matrix shape: {sim_matrix.shape}\")\n",
    "    return sim_matrix\n",
    "\n",
    "def ensure_directory_exists(directory_path):\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "def get_embedding(text):\n",
    "    # Ensure the text is a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # Tokenize and encode the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Perform inference with no gradient calculation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Move the output back to CPU, and then to numpy\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def text_to_embeddings(texts: pd.Series) -> pd.Series:\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        embedding = get_embedding(text)\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "        # Manually trigger garbage collection\n",
    "        gc.collect()\n",
    "    \n",
    "    return pd.Series(embeddings)\n",
    "\n",
    "\n",
    "def deduplicate_embeddings_sdf(spark, sdf, core_count, chunk_size, before_count, threshold, save_path=\"processed_database/\"):\n",
    "    chunk_size = 20000\n",
    "    \n",
    "    print(\"Adding embeddings to DataFrame...\")\n",
    "    sdf = sdf.withColumn(\"embeddings\", text_to_embeddings(sdf[\"conversations\"]))\n",
    "\n",
    "    print(\"Converting DataFrame to Pandas...\")\n",
    "    pdf = sdf.toPandas()\n",
    "\n",
    "    embeddings = list(tqdm(pdf['embeddings'], desc=\"Processing Embeddings\"))\n",
    "    texts = list(pdf['conversations'])\n",
    "    origins = list(pdf['origin'])\n",
    "\n",
    "    print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "\n",
    "    print(\"Calculating cosine similarity matrix...\")\n",
    "    cosine_similarity(embeddings, core_count, save_path)\n",
    "\n",
    "    print(\"Loading the full cosine similarity matrix from multiple files.\")\n",
    "    sim_matrix = load_complete_similarity_matrix(save_path)\n",
    "    \n",
    "    print(f\"Shape of similarity matrix: {sim_matrix.shape}\")\n",
    "    \n",
    "    if sim_matrix.nnz == 0:\n",
    "        raise ValueError(\"Similarity matrix is empty. Check the cosine similarity calculation.\")\n",
    "\n",
    "    print(\"Identifying duplicates based on threshold...\")\n",
    "    # Efficiently create duplicates_mask\n",
    "    duplicates_mask = sim_matrix.copy()\n",
    "    duplicates_mask.data[duplicates_mask.data < threshold] = 0\n",
    "    duplicates_mask.eliminate_zeros()\n",
    "    \n",
    "    # Remove self-similarities\n",
    "    duplicates_mask.setdiag(0)\n",
    "    duplicates_mask.eliminate_zeros()\n",
    "    \n",
    "    print(f\"Shape of duplicates_mask: {duplicates_mask.shape}\")\n",
    "    print(f\"Number of True values in duplicates_mask: {duplicates_mask.nnz}\")\n",
    "    \n",
    "    to_keep = ~(duplicates_mask.sum(axis=0) > 0).A.ravel()\n",
    "    \n",
    "    print(f\"Shape of to_keep mask: {to_keep.shape}\")\n",
    "    print(f\"Number of True values in to_keep: {np.sum(to_keep)}\")\n",
    "\n",
    "    if len(to_keep) != len(pdf):\n",
    "        print(f\"Warning: Mismatch in lengths: to_keep ({len(to_keep)}) != pdf ({len(pdf)})\")\n",
    "        print(\"Adjusting to_keep array to match pdf length...\")\n",
    "        if len(to_keep) < len(pdf):\n",
    "            to_keep = np.pad(to_keep, (0, len(pdf) - len(to_keep)), 'constant', constant_values=True)\n",
    "        else:\n",
    "            to_keep = to_keep[:len(pdf)]\n",
    "        print(f\"Adjusted to_keep length: {len(to_keep)}\")\n",
    "\n",
    "    print(\"Deduplicating DataFrame...\")\n",
    "    deduped_pdf = pdf[to_keep]\n",
    "\n",
    "    print(f\"Shape of deduped_pdf: {deduped_pdf.shape}\")\n",
    "\n",
    "    deduped_sdf = spark.createDataFrame(deduped_pdf.drop(columns=['embeddings']))\n",
    "\n",
    "    print(f\"Reduced from {len(pdf)} to {len(deduped_pdf)} entries.\")\n",
    "\n",
    "    print(\"Cleaning up temporary files...\")\n",
    "    for filename in os.listdir(save_path):\n",
    "        file_path = os.path.join(save_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "    return deduped_sdf\n",
    "    \n",
    "\n",
    "def load_chunk(filepath):\n",
    "    return load_npz(filepath)\n",
    "\n",
    "def convert_to_array(embeddings):\n",
    "    \"\"\"Convert embeddings list to a numpy array if not already an array.\"\"\"\n",
    "    if isinstance(embeddings, list):\n",
    "        return np.array(embeddings, dtype=np.float16)\n",
    "    return embeddings\n",
    "\n",
    "def shuffle_dataset(sdf):\n",
    "    \"\"\"\n",
    "    Shuffle each dataset randomly\n",
    "    \"\"\"\n",
    "    print(\"üîÄ Shuffling dataset to randomize row order...\")\n",
    "    sdf = sdf.withColumn(\"unique_id\", monotonically_increasing_id())\n",
    "    window_spec = Window.partitionBy(\"origin\").orderBy(rand())\n",
    "    sdf = sdf.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "    sdf = sdf.orderBy(\"row_num\", rand())\n",
    "    sdf = sdf.drop(\"row_num\", \"unique_id\")\n",
    "    return sdf\n",
    "\n",
    "def shuffle_final_dataset(final_sdf):\n",
    "    \"\"\"\n",
    "    Shuffle the final dataset randomly.\n",
    "    \"\"\"\n",
    "    print(\"üîÄ Shuffling final dataset to randomize row order...\")\n",
    "    return final_sdf.orderBy(rand())\n",
    "\n",
    "def sample_datasets(sdf, dataset_name, dataset_use_percentage):\n",
    "    \"\"\"\n",
    "    Samples a percentage of the dataset based on the dataset_name using a fraction.\n",
    "    \"\"\"\n",
    "    initial_count = sdf.count()\n",
    "    print(f\"üî¢ Initial row count for {dataset_name}: {initial_count}\")\n",
    "    if dataset_name in dataset_use_percentage:\n",
    "        percentage = dataset_use_percentage[dataset_name] / 100.0\n",
    "        sampled_sdf = sdf.sample(withReplacement=False, fraction=percentage, seed=42)\n",
    "        final_count = sampled_sdf.count()\n",
    "        print(f\"üìâ Row count after sampling {dataset_name}: {final_count}\")\n",
    "        return sampled_sdf\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No sampling percentage defined for {dataset_name}, using full dataset.\")\n",
    "        return sdf\n",
    "        \n",
    "def print_filtered_schema(df, exclude_field):\n",
    "    schema = df.schema\n",
    "    for field in schema:\n",
    "        if field.name != exclude_field:\n",
    "            # Check if the field is a struct type and needs further inspection\n",
    "            if isinstance(field.dataType, StructType):\n",
    "                print(f\"|-- {field.name}: struct (nullable = {field.nullable})\")\n",
    "                for subfield in field.dataType.fields:\n",
    "                    if subfield.name != exclude_field:\n",
    "                        print(f\" |    |-- {subfield.name}: {subfield.dataType.simpleString()} (nullable = {subfield.nullable})\")\n",
    "            else:\n",
    "                print(f\"|-- {field.name}: {field.dataType.simpleString()} (nullable = {field.nullable})\")\n",
    "                \n",
    "def process_and_merge_files(input_directory, temp_output_dir, final_output_file, dataset_use_percentage, core_count, chunk_size):\n",
    "    spark = initialize_spark_session(core_count)\n",
    "    print(\"üåü Initializing merging and sampling process...\")\n",
    "    \n",
    "    sampled_sdfs = []\n",
    "    for dataset_name, percentage in dataset_use_percentage.items():\n",
    "        file_pattern = os.path.join(input_directory, dataset_name.replace('/', '_') + '_chunk_*.parquet')\n",
    "        print(f\"\\nüìÇ Loading and sampling dataset {dataset_name} with file pattern: {file_pattern}\")\n",
    "        specific_sdf = spark.read.parquet(file_pattern)\n",
    "        specific_sdf = shuffle_dataset(specific_sdf)\n",
    "        sampled_sdf = sample_datasets(specific_sdf, dataset_name, dataset_use_percentage)\n",
    "        sampled_sdfs.append(sampled_sdf)\n",
    "    \n",
    "    print(\"\\nüîó Unioning all sampled dataframes...\")\n",
    "    final_sdf = sampled_sdfs[0]\n",
    "    for sdf in sampled_sdfs[1:]:\n",
    "        final_sdf = final_sdf.union(sdf)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"\\nüìã Files successfully loaded. Inspecting schema...\")\n",
    "    print_filtered_schema(final_sdf, \"token_distribution\")\n",
    "    print(\"\")\n",
    "    \n",
    "    final_sdf = shuffle_final_dataset(final_sdf)\n",
    "\n",
    "    before_count = final_sdf.count()\n",
    "    print(f\"\\n‚úÖ Before deduplication row count: {before_count}\")\n",
    "    final_sdf = adapt_schema(final_sdf)\n",
    "    final_sdf = deduplicate_embeddings_sdf(spark, final_sdf, core_count, chunk_size, before_count, threshold=0.95)\n",
    "    final_count = final_sdf.count()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Deduplication completed. Final row count: {final_count}\")\n",
    "    os.makedirs(os.path.dirname(final_output_dir), exist_ok=True)\n",
    "    \n",
    "    print(f\"üíæ Saving deduplicated data to temporary directory: {temp_output_dir}...\")\n",
    "    final_sdf.coalesce(1).write.mode('overwrite').parquet(temp_output_dir)\n",
    "    print(\"üèÅ Data saved. Preparing to consolidate into a single file...\")\n",
    "    pd_df = pd.read_parquet(temp_output_dir, engine='pyarrow')\n",
    "    pd_df.to_parquet(final_output_file, engine='pyarrow')\n",
    "    \n",
    "    print(f\"üéâ Merged data saved to: {final_output_file}\")\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = 'processed_chunks/' # Input directory\n",
    "    temp_output_dir = '/tmp/merged_data'  # Temporary directory for Spark output\n",
    "    final_output_dir = 'final/' # Final directory\n",
    "    final_output_file = 'final/merged_cleaned_shuffled_deduped.parquet'  # Final single Parquet file\n",
    "    \n",
    "    process_and_merge_files(input_directory, temp_output_dir, final_output_file, dataset_use_percentage, core_count, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9704e59b-4d3c-4883-a303-3137c4d83271",
   "metadata": {
    "id": "9704e59b-4d3c-4883-a303-3137c4d83271",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/TheSkullery/Aether-Lite-v1.8.1/commit/cc93afcaa867ad2b63f4f5a9060552728c5198cb', commit_message='Upload dataset', commit_description='', oid='cc93afcaa867ad2b63f4f5a9060552728c5198cb', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Upload To Huggingface make sure to log in\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_files=\"./final/modified_file.parquet\")\n",
    "dataset.push_to_hub(f\"[user/model]\")\n",
    "\n",
    "#created by Steel-skull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd43cb-d52c-4eda-aeeb-b8941247f27a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
